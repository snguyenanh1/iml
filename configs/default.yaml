pipeline_timeout: 28800
per_execution_timeout: 28800

# Data Perception
max_file_group_size_to_show: 5
num_example_files_to_show: 1

max_chars_per_file: 1024
num_tutorial_retrievals: 30
max_num_tutorials: 5
max_user_input_length: 2048
max_error_message_length: 2048
max_tutorial_length: 32768
create_venv: false
condense_tutorials: True
use_tutorial_summary: True

# Debug
verbosity: 4

# Default LLM Configuration
llm: &default_llm
  provider: gemini
  model: gemini-2.5-flash
  max_tokens: 16384
  proxy_url: null
  temperature: 0.0
  top_p: 0.9
  verbose: True
  multi_turn: False
  template: null
  add_coding_format_instruction: false

description_analyzer:
  <<: *default_llm

profiling_summarizer:
  <<: *default_llm

guideline_generator:
  <<: *default_llm

preprocessing_coder:
  <<: *default_llm

modeling_coder:
  <<: *default_llm

assembler:
  <<: *default_llm

hyperparameter_tuning:
  # Global default settings (fallback)
  default:
    n_trials: 50
    direction: maximize
    sampler: TPESampler(seed=42, n_startup_trials=10, n_ei_candidates=24)
    pruner: MedianPruner(n_startup_trials=5, n_warmup_steps=3, interval_steps=1)
    timeout: 7200 # 2 hours
    max_retries: 5

  # Traditional ML algorithms (CatBoost, LightGBM, XGBoost, Random Forest)
  # Training time: 1-5 minutes per trial -> Can run many trials
  traditional:
    n_trials: 150 # Realistic for 1-2 hours with pruning (avg 3min/trial Ã— 150 = 7.5h, but pruning reduces significantly)
    direction: maximize
    sampler: TPESampler(seed=42, n_startup_trials=15, n_ei_candidates=24)
    pruner: MedianPruner(n_startup_trials=10, n_warmup_steps=5, interval_steps=1)
    timeout: 7200 # 2 hours
    max_retries: 7

  # Custom Neural Networks (PyTorch/TensorFlow from scratch)
  # Training time: 2-5 hours per trial -> CRITICAL: Use fast training mode
  custom_nn:
    n_trials: 20 # Conservative - focus on fast training with reduced epochs/data
    direction: maximize
    sampler: TPESampler(seed=42, n_startup_trials=5, n_ei_candidates=24)
    # Aggressive pruning to stop poor trials early (HyperbandPruner equivalent)
    pruner: MedianPruner(n_startup_trials=2, n_warmup_steps=3, interval_steps=1)
    timeout: 7200 # 2 hours
    max_retries: 3
    # Strategy: Use reduced training for screening
    fast_training_mode: true # Flag for implementation to reduce epochs/data

  # Pretrained Models (HuggingFace, transfer learning)
  # Training time: 30min-2 hours per trial -> Moderate trials possible
  pretrained:
    n_trials: 30 # Moderate number, pretrained models fine-tune faster
    direction: maximize
    sampler: TPESampler(seed=42, n_startup_trials=8, n_ei_candidates=24)
    pruner: MedianPruner(n_startup_trials=3, n_warmup_steps=5, interval_steps=2)
    timeout: 7200 # 2 hours
    max_retries: 4
